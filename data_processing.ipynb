{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "This notebook focuses on processing and preparing medical data for predictive modeling. It includes functions for feature engineering, handling missing data, scaling features, and removing outliers. The workflow involves loading datasets, applying transformations, and saving processed data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from column_groups import *\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import zscore\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def separate_id(row):\n",
    "    \"\"\"\n",
    "    Determines if the 'epic_pmrn' value in the row contains any letters and therefore comes from numom.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): A row from the DataFrame containing 'epic_pmrn' column.\n",
    "    \n",
    "    Returns:\n",
    "        bool: Returns True if 'epic_pmrn' contains exactly one letter, False otherwise.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If 'epic_pmrn' contains more than one letter or is not a valid format.\n",
    "    \"\"\"\n",
    "    # Check if the string is all numbers\n",
    "    if str(row.epic_pmrn).isdigit():\n",
    "        return False\n",
    "    \n",
    "    # Check if the string contains exactly one letter\n",
    "    letter_count = sum(c.isalpha() for c in str(row.epic_pmrn))\n",
    "    if letter_count > 0:\n",
    "        return True\n",
    "    raise(ValueError)\n",
    "\n",
    "\n",
    "def process_hdp_labs(row):\n",
    "    \"\"\"\n",
    "    Processes laboratory and vital sign data for a single row.\n",
    "    \n",
    "    This function calculates the minimum, maximum, mean, and most recent values\n",
    "    for each laboratory and vital sign column based on the available dates and limit date.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): A row from the DataFrame containing lab and vital sign data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: The input row with additional columns for minimum, maximum, mean,\n",
    "                    and most recent values of laboratory and vital sign data.\n",
    "    \"\"\"\n",
    "    lab_cols = ['labs_glucose_serum', 'labs_calcium_serum', 'labs_magnesium_blood', 'labs_ptt', 'labs_ptinr', \n",
    "                'labs_urine_protein_24h_derived_timeseries_mg_dl_d', 'labs_urine_protein_24h_derived_timeseries_volume',\n",
    "                'labs_urine_spot_protein_timeseries', 'labs_urine_spot_creatinine_timeseries', \n",
    "                'labs_urine_protein_24h_derived_timeseries_mg_tv']\n",
    "    vital_cols = []\n",
    "\n",
    "    for val_col in vital_cols + lab_cols:\n",
    "        date_col = val_col + '_datetime'\n",
    "        if not pd.isna(row[date_col]):\n",
    "            col_array = pd.to_datetime(pd.Series(row[date_col].split('|')), infer_datetime_format=True)\n",
    "            before_date_idx = np.array(col_array < row.limit_date)\n",
    "            val_arr = np.array(str(row[val_col]).split('|'))[before_date_idx]\n",
    "            val_arr = [float(x) for x in val_arr]\n",
    "            if len(val_arr) == 0:\n",
    "                row[val_col + '_min'] = None\n",
    "                row[val_col + '_max'] = None\n",
    "                row[val_col + '_mean'] = None\n",
    "                row[val_col + '_most_recent'] = None\n",
    "            else:\n",
    "                row[val_col + '_min'] = np.min(val_arr)\n",
    "                row[val_col + '_max'] = np.max(val_arr)\n",
    "                row[val_col + '_mean'] = np.mean(val_arr)\n",
    "                row[val_col + '_most_recent'] = val_arr[-1]\n",
    "    return row\n",
    "\n",
    "\n",
    "def remove_outliers_high_missingness(dfs_to_modify, labels, entry_missing_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Removes entries from DataFrames with high percentages of missing data.\n",
    "    \n",
    "    Args:\n",
    "        dfs_to_modify (list of pd.DataFrame): List of DataFrames to be modified.\n",
    "        labels (list of pd.Series): List of labels corresponding to the DataFrames.\n",
    "        entry_missing_threshold (float): The threshold for the percentage of missing data above which rows are removed.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the modified list of DataFrames and the corresponding list of labels.\n",
    "    \"\"\"\n",
    "    new_dfs = []\n",
    "    new_labels = []\n",
    "    for i in range(len(dfs_to_modify)):\n",
    "        df = dfs_to_modify[i]\n",
    "        label = labels[i]\n",
    "        \n",
    "        entry_missing_percentages = df.isnull().mean(axis=1)\n",
    "        # Remove entries with high missingness\n",
    "        df = df[entry_missing_percentages <= entry_missing_percentages.quantile(entry_missing_threshold)]\n",
    "        label = label[entry_missing_percentages <= entry_missing_percentages.quantile(entry_missing_threshold)]\n",
    "        new_dfs.append(df)\n",
    "        new_labels.append(label)\n",
    "    return new_dfs, new_labels\n",
    "\n",
    "\n",
    "def scale_features(X_train, other_dfs, weeks, exclude_features=None):\n",
    "    \"\"\"\n",
    "    Scales features in the training DataFrame and other DataFrames using Min-Max scaling.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): The training DataFrame to be scaled.\n",
    "        other_dfs (list of pd.DataFrame): List of other DataFrames to be scaled with the same scaler.\n",
    "        weeks (int): Number of weeks used for scaling file naming.\n",
    "        exclude_features (list of str, optional): List of feature names to exclude from scaling.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the scaled training DataFrame and the scaled list of other DataFrames.\n",
    "    \"\"\"\n",
    "    if exclude_features is None:\n",
    "        exclude_features = []\n",
    "    features_to_scale = [feature for feature in X_train.columns if feature not in exclude_features]\n",
    "    features_to_exclude = exclude_features\n",
    "\n",
    "    # Scaling only the selected features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[features_to_scale]), columns=features_to_scale)\n",
    "    X_train_final = pd.concat([X_train_scaled, X_train[features_to_exclude].reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Scale and concatenate other DataFrames\n",
    "    other_dfs_final = []\n",
    "    for df in other_dfs:\n",
    "        df_scaled = pd.DataFrame(scaler.transform(df[features_to_scale]), columns=features_to_scale)\n",
    "        df_final = pd.concat([df_scaled, df[features_to_exclude].reset_index(drop=True)], axis=1)\n",
    "        other_dfs_final.append(df_final)\n",
    "\n",
    "    # Dump the scaler to a file\n",
    "    joblib.dump(scaler, f'../models/scalers/{weeks}weeks_pretraining.scaler')\n",
    "\n",
    "    return X_train_final, other_dfs_final\n",
    "\n",
    "\n",
    "def impute_median(X_train, other_dfs, exclude_features=[]):\n",
    "    \"\"\"\n",
    "    Imputes missing values in DataFrames with the median of each feature.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): The training DataFrame used to compute median values.\n",
    "        other_dfs (list of pd.DataFrame): List of other DataFrames where missing values will be imputed.\n",
    "        exclude_features (list of str): List of feature names to exclude from imputation.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the imputed training DataFrame and the imputed list of other DataFrames.\n",
    "    \"\"\"\n",
    "    if exclude_features is None:\n",
    "        exclude_features = []\n",
    "    features_to_impute = [feature for feature in X_train.columns if feature not in exclude_features]\n",
    "    \n",
    "    # Calculate the median for columns that are not excluded\n",
    "    imputing_values = X_train[features_to_impute].median()\n",
    "    X_train_final = pd.concat([X_train[features_to_impute].fillna(imputing_values).reset_index(drop=True), X_train[exclude_features].reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    other_dfs_final = []\n",
    "    for df in other_dfs:\n",
    "        df_final = pd.concat([df[features_to_impute].fillna(imputing_values).reset_index(drop=True), df[exclude_features].reset_index(drop=True)], axis=1)\n",
    "        other_dfs_final.append(df_final)\n",
    "    \n",
    "    return X_train_final, other_dfs_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns containing patient information\n",
    "info_cols = ['epic_pmrn', 'delivery_date', 'delivery_hospital', 'pregnancy_start', 'limit_date', 'unique_id']\n",
    "\n",
    "# Iterate over different weeks for processing\n",
    "for weeks in [14, 20, 24, 28, 32, 34, 36, 38]:\n",
    "    # Load the dataset for the given number of weeks\n",
    "    DATA = pd.read_csv(f'../processed_data/processing_data/{weeks}_final.csv')\n",
    "    DATA['magnesium_medication'] = DATA.magnesium_medication < DATA.limit_date\n",
    "    DATA['proteinuria'] = DATA.proteinuria < DATA.limit_date\n",
    "    \n",
    "    # Determine rows that need special handling based on ID\n",
    "    final_numom_rows = DATA.apply(separate_id, axis=1)\n",
    "    \n",
    "    # Filter data: include specific hospitals and newborn gestational age or rows identified by separate_id\n",
    "    DATA = DATA[((DATA.delivery_hospital.isin(['bwh', 'mgh', 'nwh'])) & (DATA.newborn_gestational_age.notna())) | final_numom_rows.values].reset_index(drop=True)\n",
    "    \n",
    "    # Reapply separate_id logic to filtered data\n",
    "    final_numom_rows = DATA.apply(separate_id, axis=1)\n",
    "\n",
    "    # Define feature sets\n",
    "    features = [x for x in final_numom if x not in group_cols]\n",
    "    engineered_features = [x for x in DATA.columns if '__' in x or 'coef' in x]\n",
    "    final_features = features + engineered_features + comparison_cols + info_cols\n",
    "\n",
    "    # Extract unique epic_pmnr for final_numom rows\n",
    "    final_numom_epics = DATA.epic_pmrn[final_numom_rows]\n",
    "    DATA[features + engineered_features + comparison_cols] = DATA[features + engineered_features + comparison_cols].fillna(DATA[features + engineered_features + comparison_cols].median())\n",
    "    # Split epic_pmnr into training and testing sets\n",
    "    train_epics, test_epics = train_test_split(DATA[~final_numom_rows].epic_pmrn.unique(), test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Prepare training and testing data\n",
    "    X_train, y_train = DATA[DATA.epic_pmrn.isin(train_epics)][final_features], DATA[DATA.epic_pmrn.isin(train_epics)]['all_pet_xai3_dates'].notna()\n",
    "    X_test, y_test = DATA[DATA.epic_pmrn.isin(test_epics)][final_features], DATA[DATA.epic_pmrn.isin(test_epics)]['all_pet_xai3_dates'].notna()\n",
    "    \n",
    "    # Prepare numom data\n",
    "    X_numom, y_numom = DATA[DATA.epic_pmrn.isin(final_numom_epics)][final_features], DATA[DATA.epic_pmrn.isin(final_numom_epics)]['all_pet_xai3_dates'].notna()\n",
    "    \n",
    "    # Remove outliers based on missingness\n",
    "    [X_train, X_test, X_numom], [y_train, y_test, y_numom] = remove_outliers_high_missingness([X_train, X_test, X_numom], [y_train, y_test, y_numom])\n",
    "    \n",
    "    # Save the processed data to a file\n",
    "    filepath = f\"../processed_data/modelling_data/{weeks}_data.pkl\"\n",
    "    final_file = {'train': {'X': X_train, 'y': y_train},\n",
    "                  'test': {'X': X_test, 'y': y_test},\n",
    "                  'numom': {'X': X_numom, 'y': y_numom}}\n",
    "    joblib.dump(final_file, filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
